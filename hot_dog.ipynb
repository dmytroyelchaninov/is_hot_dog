{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76325,"status":"ok","timestamp":1722032648179,"user":{"displayName":"Dmytro Yelchaninov","userId":"01481124985939177553"},"user_tz":240},"id":"7rQlzW7D7NoM","outputId":"7cca19bc-21a8-4173-a2fb-724776ec65ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15, Loss: 0.1922098556323383\n","Epoch 2/15, Loss: 0.03783556456236463\n","Epoch 3/15, Loss: 0.01235702453370213\n","Epoch 4/15, Loss: 0.005758738086586888\n","Epoch 5/15, Loss: 0.002793970336188751\n","Epoch 6/15, Loss: 0.001683603021179683\n","Epoch 7/15, Loss: 0.0011463491307375463\n","Epoch 8/15, Loss: 0.0008383270384152105\n","Epoch 9/15, Loss: 0.0006363530955329901\n","Epoch 10/15, Loss: 0.0004950884062861604\n","Epoch 11/15, Loss: 0.0003938096991426809\n","Epoch 12/15, Loss: 0.0003194870488303258\n","Epoch 13/15, Loss: 0.00026273339575627483\n","Epoch 14/15, Loss: 0.0002192551924635243\n","Epoch 15/15, Loss: 0.0001849282312025736\n","Test Accuracy: 96.00%\n"]}],"source":["import os\n","import torch\n","from torch import nn, optim\n","from torchvision import datasets, transforms\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","from torch.utils.data import DataLoader\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","train_dir = '/content/drive/MyDrive/hackathon/datasets/train'\n","test_dir = '/content/drive/MyDrive/hackathon/datasets/test'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True\n",")\n","model.classifier = nn.Linear(model.config.hidden_size, 2)\n","model.to('cuda')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","\n","epochs = 15\n","model.train()\n","\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","\n","        optimizer.zero_grad()\n","        outputs = model(images).logits\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n","\n","model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","        outputs = model(images).logits\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Test Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":794,"status":"ok","timestamp":1722032648971,"user":{"displayName":"Dmytro Yelchaninov","userId":"01481124985939177553"},"user_tz":240},"id":"R9IE5yWhChIc","outputId":"36b71505-0ff1-414c-dcaa-b3b7b6dee1d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to /content/drive/MyDrive/hackathon/fine_tuned_vit_hotdog_not_hotdog.pth\n"]}],"source":["model_save_path = \"/content/drive/MyDrive/hackathon/fine_tuned_vit_hotdog_not_hotdog.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(f\"Model saved to {model_save_path}\")"]},{"cell_type":"markdown","metadata":{"id":"tytbFOrEW5nc"},"source":["# Continue training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2768,"status":"ok","timestamp":1722041134811,"user":{"displayName":"Dmytro Yelchaninov","userId":"01481124985939177553"},"user_tz":240},"id":"7KIPQILQWv4j","outputId":"916d95ab-9a00-4f1a-d585-ceacf888090c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["ViTForImageClassification(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTSdpaAttention(\n","            (attention): ViTSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import torch\n","from torch import nn, optim\n","from torchvision import datasets, transforms\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","from torch.utils.data import DataLoader\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","model_save_path = \"/content/drive/MyDrive/hackathon/fine_tuned_vit_hotdog_not_hotdog_continue_2.pth\"\n","\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True\n",")\n","# Adjust the final layer\n","model.classifier = nn.Linear(model.config.hidden_size, 2)\n","model.load_state_dict(torch.load(model_save_path))\n","model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIqoh2DwW4MI"},"outputs":[],"source":["train_dir = '/content/drive/MyDrive/hackathon/datasets/train'\n","test_dir = '/content/drive/MyDrive/hackathon/datasets/test'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize to match ViT input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"b5LupzVBW4JW","outputId":"426c9e7d-06d5-46d3-cf6d-e9e89e60388b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 5.321841530533297e-10\n","Epoch 2/10, Loss: 5.041744782732677e-10\n","Epoch 3/10, Loss: 5.041744782732677e-10\n","Epoch 4/10, Loss: 4.761647767810729e-10\n","Epoch 5/10, Loss: 4.481550786278947e-10\n","Epoch 6/10, Loss: 4.2014537880520816e-10\n","Epoch 7/10, Loss: 4.2014537880520816e-10\n","Epoch 8/10, Loss: 3.9213570402514624e-10\n","Epoch 9/10, Loss: 3.9213570402514624e-10\n","Epoch 10/10, Loss: 3.9213570402514624e-10\n"]}],"source":["model.train()\n","\n","additional_epochs = 10\n","for epoch in range(additional_epochs):\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","\n","        optimizer.zero_grad()\n","        outputs = model(images).logits\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{additional_epochs}, Loss: {running_loss/len(train_loader)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wuD_TAwHX5lM","outputId":"019dd59c-13f6-4ea2-b859-46b00623c2aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 96.75%\n"]}],"source":["model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","        outputs = model(images).logits\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Test Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YkdDqAmGXG57","outputId":"a1ea22d6-731d-482a-cc1b-5a3ddcd28ad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to /content/drive/MyDrive/hackathon/fine_tuned_vit_hotdog_not_hotdog_continue_3.pth\n"]}],"source":["model_save_path = \"/content/drive/MyDrive/hackathon/fine_tuned_vit_hotdog_not_hotdog_continue_3.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(f\"Model saved to {model_save_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qi4RBDbAswOy"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"authorship_tag":"ABX9TyNH8RxpuNWZEb1GPl9TZnMn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}